# -*- coding: utf-8 -*-
"""Detección de neologismos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xgduDZjQ_HIF9fz_xCfnNqvxyjH7TEw-

# Librerias necesarias para el funcionamiento
"""

from collections import Counter
import re
import csv
import spacy.cli

pipeline_name = 'es_core_news_lg'

# install the pipeline
spacy.cli.download(pipeline_name)

nlp = spacy.load(pipeline_name)

"""# Leer los archivos csv"""

def abrir_csv(file_path) -> str:
    try:
        with open(file_path, 'r', encoding='utf-8', newline='') as csvfile:
            reader = csv.reader(csvfile)
            # Ignoramos las primeras cinco lineas que solo dan información de sketch engine
            for _ in range(5):
                next(reader)
            # Solo queremos guardar los contextos, no hace falta el origen, etc.
            content = ' '.join([' '.join(row[1:]) for row in reader])

            # Eliminamos elementos de formato no necesarios para el estudio
            symbol_pattern = re.compile(r'[><|]|/s><s')
            content = re.sub(symbol_pattern, '', content)

            return content
    #En caso de error
    except FileNotFoundError:
        return f"El archivo '{file_path}' no existe."
    except Exception as e:
        return f"Ha ocurrido un error: {e}"

#toxico

toxico_11_csv = "toxico_2011.csv"
toxico_22_csv = "toxico_2022.csv"
toxico_11 = abrir_csv(toxico_11_csv)
toxico_22 = abrir_csv(toxico_22_csv)
#print(toxico_22[:1000]) #comprobar formato

#unicornio

unicornio_11_csv = "unicornio_2011.csv"
unicornio_22_csv = "unicornio_2022.csv"
unicornio_11 = abrir_csv(unicornio_11_csv)
unicornio_22 = abrir_csv(unicornio_22_csv)
#print(unicornio_11[:1000])

#hibernar

hibernar_11_csv = "hibernar_2011.csv"
hibernar_22_csv = "hibernar_2022.csv"
hibernar_11 = abrir_csv(hibernar_11_csv)
hibernar_22 = abrir_csv(hibernar_22_csv)
#print(hibernar_22[:1000])


"""# Palabras más comunes"""

nlp.max_length = 3000000

#Esta función devuelve 50 sustantivos según su frecuencia junto al neologismo
def get_nouns(file, avoid_word):
    doc = nlp(file)

    lemmatized_nouns = [token.lemma_
                        for token in doc
                        if (not token.is_stop and
                            not token.is_punct and
                            token.pos_ == "NOUN" and
                            token.lemma_ != avoid_word)]

    noun_freq = Counter(lemmatized_nouns)
    common_nouns = noun_freq.most_common(50)

    print("Aquí están!\n")

    for index, (noun, frequency) in enumerate(common_nouns, start=1):
        print(f"{index}. {noun}: {frequency}")


neo = input("\n ¿Qué neologismo? (tóxico, unicornio o hibernar) \n").lower()
if neo == "tóxico":
    corpus= "toxico"
else:
    corpus = neo

corpus_11 = str(corpus) + "_11"
corpus_22 = str(corpus) + "_22"
evitar_neologismo = neo


# corpus 2011
print(f"\n Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2011...")
sustantivos_11 = get_nouns(globals()[corpus_11], evitar_neologismo)
print(sustantivos_11)

print(f"\n Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2021-22...")
sustantivos_22 = get_nouns(globals()[corpus_22], evitar_neologismo)
print(sustantivos_22)

"""# Dependencia Sintáctica"""

#Esta función coge el lemma del neologismo
def get_dep_sus(file, target_w):
    doc = nlp(file)

    target_word = target_w
    target_lemma = nlp(target_word)[0].lemma_

    linked_words = []

    for token in doc:
        if not token.is_stop and not token.is_punct and token.pos_ == "NOUN" and token.head.lemma_ == target_lemma:
            linked_words.append((token.lemma_, token.dep_))

    word_frequency = Counter(linked_words)

    print(f"Top 25 palabras relacionadas con el lemma'{target_word}': \n")
    for (word, dep), count in word_frequency.most_common(25):
        print(f"'{word}' funciona como {dep}: {count} veces")
        # Print example sentences for the word
        #print("Example sentences:")
        #example_sentence_counter = 0
        #for sent in doc.sents:
            #if word in [token.lemma_ for token in sent]:
               # print(sent.text)
                #example_sentence_counter += 1
                #if example_sentence_counter >= 2:  # Change to limit the number of example sentences per word
                   # break  # Stop after printing the specified number of example sentences
        #print()

def get_suj_neo(file, target_w):
    doc = nlp(file)

    target_word = target_w
    target_lemma = nlp(target_word)[0].lemma_

    linked_words = []
    for token in doc:
        if not token.is_stop and not token.is_punct and token.pos_ == "NOUN" and token.head.lemma_ == target_lemma and token.dep_ == "nsubj":
            linked_words.append((token.lemma_, token.dep_))
    word_frequency = Counter(linked_words)

    print(f"Top 50 palabras 'sujeto' en relacion con '{target_word}': \n")
    for (word, dep), count in word_frequency.most_common(50):
        print(f"'{word}' funciona como {dep}: {count} veces")

neologismo = evitar_neologismo

print(f"\n Buscando las dependencias sintácticas más frecuentes de {evitar_neologismo} en 2011...")
get_dep_sus(globals()[corpus_11], neologismo)

#para ver solo los sujetos
#get_suj_neo(globals()[corpus_11], neologismo)

print(f"\n Buscando las dependencias sintácticas más frecuentes de {evitar_neologismo} en 2021-22...")
get_dep_sus(globals()[corpus_22], neologismo)

#get_suj_neo(globals()[corpus_22], neologismo)