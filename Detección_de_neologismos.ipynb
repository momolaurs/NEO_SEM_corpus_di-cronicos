{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EfBfTd-k3qsh",
        "9_m2dHP7QuPv",
        "AueVrIoiQ1Kf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias necesarias para el funcionamiento"
      ],
      "metadata": {
        "id": "EfBfTd-k3qsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKWjjur_IBsr",
        "outputId": "a62ea5a7-d5cd-470c-ad03-91097dd783e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n",
            "Collecting es-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-lg==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.1.5)\n",
            "Installing collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gdown\n",
        "!python -m spacy download es_core_news_lg\n",
        "import spacy\n",
        "import es_core_news_lg\n",
        "from collections import Counter\n",
        "import re\n",
        "import csv\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargar Archivos"
      ],
      "metadata": {
        "id": "9_m2dHP7QuPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tóxico archivos csv\n",
        "\n",
        "!gdown 1gmV34z9n5URFOHBbbWtP-eNLjLWdtXNE #toxico_2011.csv\n",
        "!gdown 1YdSD8firXalyWeepu0hwuGCYo5EDj47v #toxico_2022.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUyc-7cviB-0",
        "outputId": "ee9ce416-b326-4330-fdef-b3e227897ce2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gmV34z9n5URFOHBbbWtP-eNLjLWdtXNE\n",
            "To: /content/toxico_2011.csv\n",
            "100% 2.79M/2.79M [00:00<00:00, 145MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YdSD8firXalyWeepu0hwuGCYo5EDj47v\n",
            "To: /content/toxico_2022.csv\n",
            "100% 2.83M/2.83M [00:00<00:00, 74.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unicornio archivos csv\n",
        "\n",
        "!gdown 10YiOpKeUswK74jijk2Go34cLVfN8pNa2 #unicornio_2011.csv\n",
        "!gdown 1RG_7b4IiKXR_bkH3KQTvzsajCyQmKuYh #unicornio_2022.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wls2B-b_5Ddl",
        "outputId": "1ec01ba5-e1fa-4abc-8dd2-0ad43c032c01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10YiOpKeUswK74jijk2Go34cLVfN8pNa2\n",
            "To: /content/unicornio_2011.csv\n",
            "100% 2.01M/2.01M [00:00<00:00, 78.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RG_7b4IiKXR_bkH3KQTvzsajCyQmKuYh\n",
            "To: /content/unicornio_2022.csv\n",
            "100% 2.86M/2.86M [00:00<00:00, 58.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hibernar archivos csv\n",
        "\n",
        "!gdown 1xboi6KBAuQx-Fe8iM0GVQ7i3BlNcfAMR #hibernar_2011.csv\n",
        "!gdown 1pxrNqFVR4QDcu1VlUFdbnHeGyroDNqa9 #hibernar_2022.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJG_BDIZQjrq",
        "outputId": "ed51bce8-3d01-47eb-9520-258f31887e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xboi6KBAuQx-Fe8iM0GVQ7i3BlNcfAMR\n",
            "To: /content/hibernar_2011.csv\n",
            "100% 943k/943k [00:00<00:00, 69.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pxrNqFVR4QDcu1VlUFdbnHeGyroDNqa9\n",
            "To: /content/hibernar_2022.csv\n",
            "100% 479k/479k [00:00<00:00, 48.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#viral archivos csv\n",
        "\n",
        "!gdown 15ta-Z7Li1uaIiTbiXQxCdoIa4anaf-9i #viral_2011.csv\n",
        "!gdown 1slxwNFVEyMHLQ1ZGmj6hGz7Y7CJidTKa #viral_2022.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUEERXJg5KdO",
        "outputId": "bf8dfdc7-ad54-4424-d896-f4f03cc96a89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15ta-Z7Li1uaIiTbiXQxCdoIa4anaf-9i\n",
            "To: /content/viral_2011.csv\n",
            "100% 2.80M/2.80M [00:00<00:00, 156MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1slxwNFVEyMHLQ1ZGmj6hGz7Y7CJidTKa\n",
            "To: /content/viral_2022.csv\n",
            "100% 2.84M/2.84M [00:00<00:00, 139MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leer los archivos"
      ],
      "metadata": {
        "id": "AueVrIoiQ1Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abrir_csv(file_path) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            # Ignoramos las primeras cinco lineas que solo dan información de sketch engine\n",
        "            for _ in range(5):\n",
        "                next(reader)\n",
        "            # Solo queremos guardar los contextos, no hace falta el origen, etc.\n",
        "            content = ' '.join([' '.join(row[1:]) for row in reader])\n",
        "\n",
        "            # Eliminamos elementos de formato no necesarios para el estudio\n",
        "            symbol_pattern = re.compile(r'[><|]|/s><s')\n",
        "            content = re.sub(symbol_pattern, '', content)\n",
        "\n",
        "            return content\n",
        "    #En caso de error\n",
        "    except FileNotFoundError:\n",
        "        return f\"El archivo '{file_path}' no existe.\"\n",
        "    except Exception as e:\n",
        "        return f\"Ha ocurrido un error: {e}\""
      ],
      "metadata": {
        "id": "RbVswHBPJ2ms"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#toxico\n",
        "\n",
        "toxico_11_csv = \"toxico_2011.csv\"\n",
        "toxico_22_csv = \"toxico_2022.csv\"\n",
        "toxico_11 = abrir_csv(toxico_11_csv)\n",
        "toxico_22 = abrir_csv(toxico_22_csv)\n",
        "print(toxico_22[:1000]) #comprobar formato"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwwAvSL5Jbwm",
        "outputId": "ced1a8f3-068a-4f90-c517-96c1364648a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s A su juicio, la corrupción forma parte de una cultura social.  El corrupto es un depredador de la sociedad, tiene hábitos tóxicos , es deshonesto, no conoce límites ni ve consecuencias de sus actos, analiza.  Esta patología de tipo social tiene daños .  La contaminación del aire ambiente -incluidos los contaminantes potencialmente dañinos como las PM2,5 y los gases tóxicos emitidos por las industrias, los hogares y los vehículos- puede aumentar la inflamación y el estrés oxidativo en el ), obesidad, sedentarismo, tabaquismo, enfermedades del corazón que alteran el ritmo cardíaco así como el consumo de tóxicos y la toma de anticonceptivos orales.  Los especialistas advierten que también se dan ictus en gente joven.  Ocurren por de tomar suplementos.  Ahora bien, como recuerda Piñeiro, \"no se puede obviar que la vitamina D en altas dosis es tóxica \".  En opinión de López, esta ''sobreimportancia'' a la vitamina D puede deberse a que \"todo lo que rodea al mundo del hueso, para experimen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unicornio\n",
        "\n",
        "unicornio_11_csv = \"unicornio_2011.csv\"\n",
        "unicornio_22_csv = \"unicornio_2022.csv\"\n",
        "unicornio_11 = abrir_csv(unicornio_11_csv)\n",
        "unicornio_22 = abrir_csv(unicornio_22_csv)\n",
        "print(unicornio_11[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIBKCHm52sgu",
        "outputId": "285e1027-eec6-459d-ea74-090ef14a6205"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", de lento caminar, entre la calima del desierto, hizo pensar a muchos viajeros que se encontraban ante el buscado unicornio .  En el siglo IV antes de Cristo, Aristóteles ya asociaba el oryx de Arabia (Oryx leucoryx) con la mítica criatura. /s su existencia, por lo tanto han pasado a ser considerados personajes de leyenda o ficción, como lo son los unicornios , el Fary o Spock.  ¿Sabías que...?  ...los pelos de polla son muy poco nutritivos ?  Te puedes comer un_millón y no engordas los que luchan por MonEsVol .  Ahí entra cualquiera que luche por Dios, principalmente en contra de la Inciclopedia y su Unicornio Rosa Invisible.  Dante se encuentra en este lugar con su tatarabuelo, quién le dice que lo van a correr de su casa por vestuario de la peli \"Las vírgenes suicidas\" y en la foto no se ve, pero lo que me ha ganado definitivamente es que tiene un unicornio blanco en la parte de atrás.  Puede llegar a ser tan reconocible como el bikini de la piña de Chloé de hace algunos años. /s c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hibernar\n",
        "\n",
        "hibernar_11_csv = \"hibernar_2011.csv\"\n",
        "hibernar_22_csv = \"hibernar_2022.csv\"\n",
        "hibernar_11 = abrir_csv(hibernar_11_csv)\n",
        "hibernar_22 = abrir_csv(hibernar_22_csv)\n",
        "print(hibernar_22[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m96tFFk-Q9T6",
        "outputId": "0d42f088-39ba-48f2-fb11-8915f1adee21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alsa, IAG 7 y Gowall.  El Gobierno confirmó en septiembre la suspensión de los viajes del Imserso.  Llevaban varios meses hibernados por la crisis sanitaria.  Este año, al no haber concluido la campaña de vacunación y al no tener controlado el .  Al ejecutar el archivo aparecerá una ventana con muchas acciones a realizar: apagar, reiniciar, suspender, hibernar , bloquear y cerrar la sesión.  El proceso de configuración es el mismo para todas las opciones así que vamos a probar con la mayor parte de su existencia en Estados Unidos y Canadá, y en época invernal migra a tierras mexicanas; para poder hibernar recorre alrededor de 4_mil_200 kilómetros en esta travesía y descansa en los cálidos bosques de oyamel, pino, encino y en griego significa \"transformación somnolienta\".  Este nombre hace referencia a la capacidad que tiene la especie de hibernar y transformarse de oruga a una bella mariposa color naranja vibrante.  Las mariposas nacidas durante este periodo de agua.  En lo que respect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#viral\n",
        "\n",
        "viral_11_csv = \"viral_2011.csv\"\n",
        "viral_22_csv = \"viral_2022.csv\"\n",
        "viral_11 = abrir_csv(viral_11_csv)\n",
        "viral_22 = abrir_csv(viral_22_csv)\n",
        "print(viral_22[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SXzqxrI34hj",
        "outputId": "8b10c18a-f1eb-4a04-bc71-44fee9df4621"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rottweiler reciben más de tres dosis de la vacuna polivalente (V8 o V10) ya que su predisposición a enfermedades virales es tan grande.  El Rottweiler tiene una reputación de ser valiente, pero lo que pocos saben es que es un excelente perro de .  \"Porque es mi vida privada, no pública\", lo que deja ver un poco de molestia del cantante.  El comentario se volvió viral inmediatamente y fue replicado por páginas en redes que se encargan de seguirle los pasos a las personalidades de la .  Seguramente con un producto a base de mRna incluso para aquellos que ya han sido inmunizados con una vacuna de vector viral como AstraZeneca o Janssen.  Ulloa: resistencia a cambiar, el gran problema de Panamá  El arzobispo José Domingo Ulloa se posaran en él.  Dibu fue héroe de la Selección con un equilibrio justo entre sus magníficas condiciones, un trash talk viral y algunos métodos alternativos para un futbolista como el yoga, los pilates o la psicología.  A la tradicional que una persona se infecta c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Palabras más comunes"
      ],
      "metadata": {
        "id": "QTIpfoUxDXBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_lg.load()\n",
        "nlp.max_length = 3000000\n",
        "\n",
        "#Esta función devuelve 50 palabras de todo tipo (excepto palabras vacías y puntuación) según su frecuencia junto al neologismo\n",
        "def get_words(file):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    lemmatized_words = [token.lemma_\n",
        "                        for token in doc\n",
        "                        if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    word_freq = Counter(lemmatized_words)\n",
        "    common_words = word_freq.most_common(50)\n",
        "    return common_words\n",
        "\n",
        "#Muy similar a la anterior pero en este caso solo sustantivos\n",
        "def get_nouns(file, avoid_word):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    lemmatized_nouns = [token.lemma_\n",
        "                        for token in doc\n",
        "                        if (not token.is_stop and\n",
        "                            not token.is_punct and\n",
        "                            token.pos_ == \"NOUN\" and\n",
        "                            token.lemma_ != avoid_word)]\n",
        "\n",
        "    noun_freq = Counter(lemmatized_nouns)\n",
        "    common_nouns = noun_freq.most_common(50)\n",
        "\n",
        "    # Enumerate and print the results\n",
        "    for index, (noun, frequency) in enumerate(common_nouns, start=1):\n",
        "        print(f\"{index}. {noun}: {frequency}\")"
      ],
      "metadata": {
        "id": "c8_Yh_8OIh19"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para cambiar de neologismo solo eliminar el término en las casilla\n",
        "# a la derecha llamada neo\n",
        "#Opciones: tóxico, unicornio, hibernar\n",
        "\n",
        "\n",
        "neo = \"hibernar\" # @param {type:\"string\"}\n",
        "if neo == \"tóxico\":\n",
        "    corpus = \"toxico\"\n",
        "elif neo == \"hibernar\" or neo == \"unicornio\":\n",
        "    corpus = neo\n",
        "else:\n",
        "    print(\"Por favor, introduzca un neologismo válido.\")\n",
        "\n",
        "corpus_11 = str(corpus) + \"_11\"\n",
        "corpus_22 = str(corpus) + \"_22\"\n",
        "evitar_neologismo = neo"
      ],
      "metadata": {
        "id": "NkaubkL-L36J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus 2011\n",
        "print(f\"Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2011...\")\n",
        "sustantivos_11 = get_nouns(globals()[corpus_11], evitar_neologismo)\n",
        "print(sustantivos_11)"
      ],
      "metadata": {
        "id": "cGSeJEYfIevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus 2021-22\n",
        "print(f\"Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2021-22...\")\n",
        "sustantivos_22 = get_nouns(globals()[corpus_22], evitar_neologismo)\n",
        "print(sustantivos_22)"
      ],
      "metadata": {
        "id": "JKaZtmG6JgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencia Sintáctica"
      ],
      "metadata": {
        "id": "NaPBHXZ1UKNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta función coge el lemma del neologismo\n",
        "def get_dep_sus(file, target_w):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_word = target_w\n",
        "    target_lemma = nlp(target_word)[0].lemma_\n",
        "\n",
        "    linked_words = []\n",
        "\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\" and token.head.lemma_ == target_lemma:\n",
        "            linked_words.append((token.lemma_, token.dep_))\n",
        "\n",
        "    word_frequency = Counter(linked_words)\n",
        "\n",
        "    print(f\"Top 25 palabras relacionadas con el lemma'{target_word}': \\n\")\n",
        "    for (word, dep), count in word_frequency.most_common(25):\n",
        "        print(f\"'{word}' funciona como {dep}: {count} veces\")\n",
        "        # Print example sentences for the word\n",
        "        #print(\"Example sentences:\")\n",
        "        #example_sentence_counter = 0\n",
        "        #for sent in doc.sents:\n",
        "            #if word in [token.lemma_ for token in sent]:\n",
        "               # print(sent.text)\n",
        "                #example_sentence_counter += 1\n",
        "                #if example_sentence_counter >= 2:  # Change to limit the number of example sentences per word\n",
        "                   # break  # Stop after printing the specified number of example sentences\n",
        "        #print()\n",
        "\n",
        "def get_suj_neo(file, target_w):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_word = target_w\n",
        "    target_lemma = nlp(target_word)[0].lemma_\n",
        "\n",
        "    linked_words = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\" and token.head.lemma_ == target_lemma and token.dep_ == \"nsubj\":\n",
        "            linked_words.append((token.lemma_, token.dep_))\n",
        "    word_frequency = Counter(linked_words)\n",
        "\n",
        "    print(f\"Top 25 palabras 'sujeto' en relacion con '{target_word}': \\n\")\n",
        "    for (word, dep), count in word_frequency.most_common(25):\n",
        "        print(f\"'{word}' funciona como {dep}: {count} veces\")"
      ],
      "metadata": {
        "id": "o5SIWd7zENYW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neologismo = neo"
      ],
      "metadata": {
        "id": "DdB9imyd7BwL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_dep_sus(globals()[corpus_11], neologismo)"
      ],
      "metadata": {
        "id": "cXb7AcGJ6c2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#esta funcion sirve para comprobar solamente los sustantivos etiquetados con nsubj\n",
        "\n",
        "#get_suj_neo(globals()[corpus_11], neologismo)"
      ],
      "metadata": {
        "id": "NmFkzdD44TFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_dep_sus(globals()[corpus_22], neologismo)"
      ],
      "metadata": {
        "id": "Zf8JCz4EDSFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_suj_neo(globals()[corpus_22], neologismo)"
      ],
      "metadata": {
        "id": "ufKLlTLB65k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se ha usado esta función para buscar contextos donde ocurran una palabra especifica y el neologismo\n",
        "def checker(file, target_w1, target_w2):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_lemma1 = nlp(target_w1)[0].lemma_\n",
        "    target_lemma2 = nlp(target_w2)[0].lemma_\n",
        "\n",
        "    linked_sentences = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        word_found1 = False\n",
        "        word_found2 = False\n",
        "        for token in sent:\n",
        "            if token.lemma_ == target_lemma1:\n",
        "                word_found1 = True\n",
        "            if token.lemma_ == target_lemma2:\n",
        "                word_found2 = True\n",
        "        if word_found1 and word_found2:\n",
        "            linked_sentences.append(sent.text)\n",
        "\n",
        "    print(f\"Contextos que incluyen '{target_w1}' y '{target_w2}' :\\n\")\n",
        "    for sentence in linked_sentences:\n",
        "        print(sentence)\n"
      ],
      "metadata": {
        "id": "O0LCY-t0MYbZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabra = \"persona\" #@param {type:\"string\"}\n",
        "checker(globals()[corpus_22], neologismo, palabra)"
      ],
      "metadata": {
        "id": "DBSkaKSrNM14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}