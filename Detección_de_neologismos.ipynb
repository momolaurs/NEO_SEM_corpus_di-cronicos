{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EfBfTd-k3qsh",
        "9_m2dHP7QuPv",
        "AueVrIoiQ1Kf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias necesarias para el funcionamiento"
      ],
      "metadata": {
        "id": "EfBfTd-k3qsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKWjjur_IBsr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gdown\n",
        "!python -m spacy download es_core_news_lg\n",
        "import spacy\n",
        "import es_core_news_lg\n",
        "from collections import Counter\n",
        "import re\n",
        "import csv\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargar Archivos"
      ],
      "metadata": {
        "id": "9_m2dHP7QuPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tóxico archivos csv\n",
        "\n",
        "!gdown 1gmV34z9n5URFOHBbbWtP-eNLjLWdtXNE #toxico_2011.csv\n",
        "!gdown 1YdSD8firXalyWeepu0hwuGCYo5EDj47v #toxico_2022.csv"
      ],
      "metadata": {
        "id": "UUyc-7cviB-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unicornio archivos csv\n",
        "\n",
        "!gdown 10YiOpKeUswK74jijk2Go34cLVfN8pNa2 #unicornio_2011.csv\n",
        "!gdown 1RG_7b4IiKXR_bkH3KQTvzsajCyQmKuYh #unicornio_2022.csv"
      ],
      "metadata": {
        "id": "Wls2B-b_5Ddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hibernar archivos csv\n",
        "\n",
        "!gdown 1xboi6KBAuQx-Fe8iM0GVQ7i3BlNcfAMR #hibernar_2011.csv\n",
        "!gdown 1pxrNqFVR4QDcu1VlUFdbnHeGyroDNqa9 #hibernar_2022.csv"
      ],
      "metadata": {
        "id": "BJG_BDIZQjrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leer los archivos"
      ],
      "metadata": {
        "id": "AueVrIoiQ1Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abrir_csv(file_path) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            # Ignoramos las primeras cinco lineas que solo dan información de sketch engine\n",
        "            for _ in range(5):\n",
        "                next(reader)\n",
        "            # Solo queremos guardar los contextos, no hace falta el origen, etc.\n",
        "            content = ' '.join([' '.join(row[1:]) for row in reader])\n",
        "\n",
        "            # Eliminamos elementos de formato no necesarios para el estudio\n",
        "            symbol_pattern = re.compile(r'[><|]|/s><s')\n",
        "            content = re.sub(symbol_pattern, '', content)\n",
        "\n",
        "            return content\n",
        "    #En caso de error\n",
        "    except FileNotFoundError:\n",
        "        return f\"El archivo '{file_path}' no existe.\"\n",
        "    except Exception as e:\n",
        "        return f\"Ha ocurrido un error: {e}\""
      ],
      "metadata": {
        "id": "RbVswHBPJ2ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#toxico\n",
        "\n",
        "toxico_11_csv = \"toxico_2011.csv\"\n",
        "toxico_22_csv = \"toxico_2022.csv\"\n",
        "toxico_11 = abrir_csv(toxico_11_csv)\n",
        "toxico_22 = abrir_csv(toxico_22_csv)\n",
        "print(toxico_22[:1000]) #comprobar formato"
      ],
      "metadata": {
        "id": "YwwAvSL5Jbwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unicornio\n",
        "\n",
        "unicornio_11_csv = \"unicornio_2011.csv\"\n",
        "unicornio_22_csv = \"unicornio_2022.csv\"\n",
        "unicornio_11 = abrir_csv(unicornio_11_csv)\n",
        "unicornio_22 = abrir_csv(unicornio_22_csv)\n",
        "print(unicornio_11[:1000])"
      ],
      "metadata": {
        "id": "kIBKCHm52sgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hibernar\n",
        "\n",
        "hibernar_11_csv = \"hibernar_2011.csv\"\n",
        "hibernar_22_csv = \"hibernar_2022.csv\"\n",
        "hibernar_11 = abrir_csv(hibernar_11_csv)\n",
        "hibernar_22 = abrir_csv(hibernar_22_csv)\n",
        "print(hibernar_22[:1000])"
      ],
      "metadata": {
        "id": "m96tFFk-Q9T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Palabras más comunes"
      ],
      "metadata": {
        "id": "QTIpfoUxDXBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = es_core_news_lg.load()\n",
        "nlp.max_length = 3000000\n",
        "\n",
        "#Esta función devuelve 50 palabras de todo tipo (excepto palabras vacías y puntuación) según su frecuencia junto al neologismo\n",
        "def get_words(file):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    lemmatized_words = [token.lemma_\n",
        "                        for token in doc\n",
        "                        if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    word_freq = Counter(lemmatized_words)\n",
        "    common_words = word_freq.most_common(50)\n",
        "    return common_words\n",
        "\n",
        "#Muy similar a la anterior pero en este caso solo sustantivos\n",
        "def get_nouns(file, avoid_word):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    lemmatized_nouns = [token.lemma_\n",
        "                        for token in doc\n",
        "                        if (not token.is_stop and\n",
        "                            not token.is_punct and\n",
        "                            token.pos_ == \"NOUN\" and\n",
        "                            token.lemma_ != avoid_word)]\n",
        "\n",
        "    noun_freq = Counter(lemmatized_nouns)\n",
        "    common_nouns = noun_freq.most_common(50)\n",
        "\n",
        "    # Enumerate and print the results\n",
        "    for index, (noun, frequency) in enumerate(common_nouns, start=1):\n",
        "        print(f\"{index}. {noun}: {frequency}\")"
      ],
      "metadata": {
        "id": "c8_Yh_8OIh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para cambiar de neologismo solo eliminar el término en las casilla\n",
        "# a la derecha llamada neo\n",
        "#Opciones: tóxico, unicornio, hibernar\n",
        "\n",
        "\n",
        "neo = \"hibernar\" # @param {type:\"string\"}\n",
        "if neo == \"tóxico\":\n",
        "    corpus = \"toxico\"\n",
        "elif neo == \"hibernar\" or neo == \"unicornio\":\n",
        "    corpus = neo\n",
        "else:\n",
        "    print(\"Por favor, introduzca un neologismo válido.\")\n",
        "\n",
        "corpus_11 = str(corpus) + \"_11\"\n",
        "corpus_22 = str(corpus) + \"_22\"\n",
        "evitar_neologismo = neo"
      ],
      "metadata": {
        "id": "NkaubkL-L36J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus 2011\n",
        "print(f\"Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2011...\")\n",
        "sustantivos_11 = get_nouns(globals()[corpus_11], evitar_neologismo)\n",
        "print(sustantivos_11)"
      ],
      "metadata": {
        "id": "cGSeJEYfIevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus 2021-22\n",
        "print(f\"Buscando los sustantivos más frecuentes en el contexto general de {evitar_neologismo} en 2021-22...\")\n",
        "sustantivos_22 = get_nouns(globals()[corpus_22], evitar_neologismo)\n",
        "print(sustantivos_22)"
      ],
      "metadata": {
        "id": "JKaZtmG6JgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencia Sintáctica"
      ],
      "metadata": {
        "id": "NaPBHXZ1UKNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta función coge el lemma del neologismo\n",
        "def get_dep_sus(file, target_w):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_word = target_w\n",
        "    target_lemma = nlp(target_word)[0].lemma_\n",
        "\n",
        "    linked_words = []\n",
        "\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\" and token.head.lemma_ == target_lemma:\n",
        "            linked_words.append((token.lemma_, token.dep_))\n",
        "\n",
        "    word_frequency = Counter(linked_words)\n",
        "\n",
        "    print(f\"Top 25 palabras relacionadas con el lemma'{target_word}': \\n\")\n",
        "    for (word, dep), count in word_frequency.most_common(25):\n",
        "        print(f\"'{word}' funciona como {dep}: {count} veces\")\n",
        "        # Print example sentences for the word\n",
        "        #print(\"Example sentences:\")\n",
        "        #example_sentence_counter = 0\n",
        "        #for sent in doc.sents:\n",
        "            #if word in [token.lemma_ for token in sent]:\n",
        "               # print(sent.text)\n",
        "                #example_sentence_counter += 1\n",
        "                #if example_sentence_counter >= 2:  # Change to limit the number of example sentences per word\n",
        "                   # break  # Stop after printing the specified number of example sentences\n",
        "        #print()\n",
        "\n",
        "def get_suj_neo(file, target_w):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_word = target_w\n",
        "    target_lemma = nlp(target_word)[0].lemma_\n",
        "\n",
        "    linked_words = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\" and token.head.lemma_ == target_lemma and token.dep_ == \"nsubj\":\n",
        "            linked_words.append((token.lemma_, token.dep_))\n",
        "    word_frequency = Counter(linked_words)\n",
        "\n",
        "    print(f\"Top 25 palabras 'sujeto' en relacion con '{target_word}': \\n\")\n",
        "    for (word, dep), count in word_frequency.most_common(25):\n",
        "        print(f\"'{word}' funciona como {dep}: {count} veces\")"
      ],
      "metadata": {
        "id": "o5SIWd7zENYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neologismo = neo"
      ],
      "metadata": {
        "id": "DdB9imyd7BwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_dep_sus(globals()[corpus_11], neologismo)"
      ],
      "metadata": {
        "id": "cXb7AcGJ6c2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#esta funcion sirve para comprobar solamente los sustantivos etiquetados con nsubj\n",
        "\n",
        "#get_suj_neo(globals()[corpus_11], neologismo)"
      ],
      "metadata": {
        "id": "NmFkzdD44TFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_dep_sus(globals()[corpus_22], neologismo)"
      ],
      "metadata": {
        "id": "Zf8JCz4EDSFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_suj_neo(globals()[corpus_22], neologismo)"
      ],
      "metadata": {
        "id": "ufKLlTLB65k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#se ha usado esta función para buscar contextos donde ocurran una palabra especifica y el neologismo\n",
        "def checker(file, target_w1, target_w2):\n",
        "    doc = nlp(file)\n",
        "\n",
        "    target_lemma1 = nlp(target_w1)[0].lemma_\n",
        "    target_lemma2 = nlp(target_w2)[0].lemma_\n",
        "\n",
        "    linked_sentences = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        word_found1 = False\n",
        "        word_found2 = False\n",
        "        for token in sent:\n",
        "            if token.lemma_ == target_lemma1:\n",
        "                word_found1 = True\n",
        "            if token.lemma_ == target_lemma2:\n",
        "                word_found2 = True\n",
        "        if word_found1 and word_found2:\n",
        "            linked_sentences.append(sent.text)\n",
        "\n",
        "    print(f\"Contextos que incluyen '{target_w1}' y '{target_w2}' :\\n\")\n",
        "    for sentence in linked_sentences:\n",
        "        print(sentence)\n"
      ],
      "metadata": {
        "id": "O0LCY-t0MYbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabra = \"persona\" #@param {type:\"string\"}\n",
        "checker(globals()[corpus_22], neologismo, palabra)"
      ],
      "metadata": {
        "id": "DBSkaKSrNM14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}